== Pre-requisites
.. JDK8+
.. Locally running MySQL server
.. Locally set up of zookeeper, kafka broker and kafka connect


== Objective
To understand how to set up KAFKA souce sink connector using Avro format using spring-cloud-stream-binder-kafka

== Overview

This is a simple project which has three parts

* *Debezium source connector configuration for MySQL source*
+
`avro-mysql-source-debezium-connector` file contains configuration for source connector, important attributes of the configuration are added and explained below
+
[source, json]
{
  "name": "deb-source-connector",
  "config": {
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",                           [1]
      "value.converter": "io.confluent.connect.avro.AvroConverter",                             [2]
      "value.converter.schema.registry.url": "http://localhost:8081",                           [3]
      "database.history.kafka.bootstrap.servers": "localhost:9092",                             [4]
      "include.schema.changes": false,                                                          [5]
      "transforms": "unwrap,AddNamespace,ExtractField",                                         [6]
      "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
      "transforms.unwrap.drop.tombstones": false,                                               [7]
      "transforms.unwrap.delete.handling.mode": "rewrite",                                      [8]
      "transforms.AddNamespace.type": "org.apache.kafka.connect.transforms.SetSchemaMetadata$Value",
      "transforms.AddNamespace.schema.name": "com.example.model.Customer",                      [9]
      "table.ignore.builtin": true,
      "database.whitelist": "poc"
  }
}
+
[cols="2"]
|===
| ^[1]^
| Converter class handling logic to convert the record from source DB into a bytearray in JSON format.

| ^[2]^ 
| Converter class handling logic to convert the record from source DB into a bytearray in AVRO format.

| ^[3]^
| Confluent schema registry url

| ^[4]^
| host:port details for the KAFKA broker

| ^[5]^
| Exclude schema changes as events to the topic

| ^[6]^
| Transformations to be applied before producing message to topic, a comma seperated list of default supported transformations 

| ^[7]^
| Tombstone records are records generated by connector with key for the row that was deleted from database and null value. to avoid having these records set this to false

| ^[8]^
| To set a flag when records get deleted, an attribute of __deleted is added to the record.

| ^[9]^
| Add the namespace to use for the record so that it becomes easier to map kafka events onto POJO objects in the Streams.
|===
+
* *Streams API using spring-cloud-stream-binder-kafka*
+
There are three sections to be understood in the code:
+
** AVRO schema files
+
`curatedcustomer.avsc` and `customer.avsc` files are avro schema files, we use these files to generate `CuratedCustomer` and `Customer` classes.
** Bean for stream processing 
+
We have defined a bean named `curateCustomer` in `Curator` class which returns a `java.util.Function`. This function is essentially responsible for accepting the input stream of events and processing those to retun another stream of events which will be pushed to another topic.

** Spring boot configuration

* *JDBC sink connector configuration for MySQL destination*
+
`avro-mysql-sink-jdbc-connector` file contains configuration for sink connector, important attributes of the configuration are added and explained below
[source, json]
{
  "name": "curatedcustomer-sink",
  "config" : {
    "topics": "curatedcustomer",
    "connection.url": "jdbc:mysql://localhost:3306/curated",                           [1]
    "insert.mode": "upsert",                                                           [2]
    "table.name.format": "${topic}",                                                   [3]
    "pk.mode": "record_key",                                                           [4]
    "pk.fields" : "customerNumber",
    "auto.create": "true",
    "auto.evolve": "true"
  }
}
+
[cols="2"]
|===
| ^[1]^
| JDBC URL for the database along with destination schema name

| ^[2]^ 
| Selecting upsert as `insert.mode` would help maintain the integrity of the data from source i.e. sink connector will update records on the destination based on record key attribute selected for primary key.

| ^[3]^
| Name to give to the table in which records are to be dumped.

| ^[4]^
| How to select primary key for the table being created
|===
